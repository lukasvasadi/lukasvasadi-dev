import{s as S,n as F}from"./scheduler.ae1baad1.js";import{S as B,i as J,g as s,s as o,H as K,r as O,h as i,z as c,c as p,j as Q,D as W,f as e,u as X,k as Y,a,v as Z,d as tt,t as nt,w as et}from"./index.171b1fb8.js";import{T as at}from"./tag.e008a716.js";function st(E){let l,M='<img src="/images/nvidia-gpu.jpg" alt="NVIDIA graphics cards"/>',x,r,D="Contents",y,u,j='<li><a href="#introduction">Introduction</a></li> <li><a href="#compiling-for-the-cpu">Compiling for the CPU</a></li>',_,m,N='<a id="introduction">Introduction</a>',w,d,$="CUDA is a C/C++ API that allows developers to access the GPU for accelerated computing. For Python, the most performant option to target the GPU is the <code>pyCUDA</code> library, which exposes the entire CUDA API, but requires writing C code in docstrings. An attractive alternative is using the <code>Numba</code> library, which provides a <strong><em>just-in-time</em></strong> compiling function decorator. This allows the optimization of certain functions that handle data processing without changing the entire program, and without mixing C source code.",b,h,q="Requirements",H,f,G="<li>NVIDIA GPU with CUDA support</li> <li>Python ≥3.4</li> <li>NumPy ≥1.10</li>",P,k,V='<a id="compiling-for-the-cpu">Compiling for the CPU</a>',T,g,z="In addition to compiling Python functions for the GPU, by default, Numba targets the CPU.",U,C,I,R=`<code class="language-python"><span class="token keyword">from</span> numba <span class="token keyword">import</span> jit
<span class="token keyword">import</span> math

<span class="token decorator annotation punctuation">@jit</span>
<span class="token keyword">def</span> <span class="token function">hypot</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Implementation from https://en.wikipedia.org/wiki/Hypot</span>
    x <span class="token operator">=</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    y <span class="token operator">=</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>
    t <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    x <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    t <span class="token operator">=</span> t <span class="token operator">/</span> x
    <span class="token keyword">return</span> x <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span>t<span class="token operator">*</span>t<span class="token punctuation">)</span></code>`,A,v,L;return v=new at({props:{msg:"Depending on the GPU, the performance of calculations with <code>float32</code> and <code>float64</code> data types can be significantly different. If the calculation does not require 64-bit precision, NVIDIA recommends using <code>float32</code>. (Most modern operating systems default the value of floating point numbers to 64-bit!)"}}),{c(){l=s("p"),l.innerHTML=M,x=o(),r=s("h2"),r.textContent=D,y=o(),u=s("ul"),u.innerHTML=j,_=o(),m=s("h2"),m.innerHTML=N,w=o(),d=s("p"),d.innerHTML=$,b=o(),h=s("h3"),h.textContent=q,H=o(),f=s("ul"),f.innerHTML=G,P=o(),k=s("h2"),k.innerHTML=V,T=o(),g=s("p"),g.textContent=z,U=o(),C=s("pre"),I=new K(!1),A=o(),O(v.$$.fragment),this.h()},l(t){l=i(t,"P",{"data-svelte-h":!0}),c(l)!=="svelte-hte7b2"&&(l.innerHTML=M),x=p(t),r=i(t,"H2",{"data-svelte-h":!0}),c(r)!=="svelte-jpxk5s"&&(r.textContent=D),y=p(t),u=i(t,"UL",{"data-svelte-h":!0}),c(u)!=="svelte-n2dl39"&&(u.innerHTML=j),_=p(t),m=i(t,"H2",{"data-svelte-h":!0}),c(m)!=="svelte-117zo8l"&&(m.innerHTML=N),w=p(t),d=i(t,"P",{"data-svelte-h":!0}),c(d)!=="svelte-1pga5c9"&&(d.innerHTML=$),b=p(t),h=i(t,"H3",{"data-svelte-h":!0}),c(h)!=="svelte-uvqqyo"&&(h.textContent=q),H=p(t),f=i(t,"UL",{"data-svelte-h":!0}),c(f)!=="svelte-1bwh0wc"&&(f.innerHTML=G),P=p(t),k=i(t,"H2",{"data-svelte-h":!0}),c(k)!=="svelte-1ji3g9s"&&(k.innerHTML=V),T=p(t),g=i(t,"P",{"data-svelte-h":!0}),c(g)!=="svelte-vpt7ip"&&(g.textContent=z),U=p(t),C=i(t,"PRE",{class:!0});var n=Q(C);I=W(n,!1),n.forEach(e),A=p(t),X(v.$$.fragment,t),this.h()},h(){I.a=null,Y(C,"class","language-python")},m(t,n){a(t,l,n),a(t,x,n),a(t,r,n),a(t,y,n),a(t,u,n),a(t,_,n),a(t,m,n),a(t,w,n),a(t,d,n),a(t,b,n),a(t,h,n),a(t,H,n),a(t,f,n),a(t,P,n),a(t,k,n),a(t,T,n),a(t,g,n),a(t,U,n),a(t,C,n),I.m(R,C),a(t,A,n),Z(v,t,n),L=!0},p:F,i(t){L||(tt(v.$$.fragment,t),L=!0)},o(t){nt(v.$$.fragment,t),L=!1},d(t){t&&(e(l),e(x),e(r),e(y),e(u),e(_),e(m),e(w),e(d),e(b),e(h),e(H),e(f),e(P),e(k),e(T),e(g),e(U),e(C),e(A)),et(v,t)}}}const lt={title:"Accelerated computing with CUDA and Python",description:"Enhance data science with rapid parallel processing.",date:"2023-8-31",image:"/images/nvidia-gpu.jpg",categories:["NVIDIA","CUDA","Numba"],published:!1};class ct extends B{constructor(l){super(),J(this,l,null,st,S,{})}}export{ct as default,lt as metadata};
